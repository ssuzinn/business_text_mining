{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from kiwipiepy import Kiwi\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(name):\n",
    "    with open(f'crawl_result/{name}.json','r',encoding='utf-8')as f:\n",
    "        data=json.load(f)\n",
    "        DF=pd.DataFrame(data['data'])\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA=data_load('wineQ&A_text')\n",
    "rec=data_load('wine_recommend_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text=emoji_pattern.sub(r'', text)\n",
    "    re_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),|]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    text = re.sub(re_pattern, 'url', text)\n",
    "    text = re.sub('\\([^)]*\\)', ',', text)\n",
    "    text = re.sub('\\[[^)]*\\]', ',', text)\n",
    "    text=re.sub('[^ㄱ-ㅎ ㅏ-ㅣ가-힣A-Za-z0-9!?.,~]+',' ',text)\n",
    "    text=re.sub('[\\\\s+ *]',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(DF):\n",
    "    DF['clean_content']=DF.본문.apply(lambda x:clean_text(x))\n",
    "    DF['clean_title']=DF.제목.apply(lambda x:clean_text(x))\n",
    "    DF['contents']=DF.clean_title+DF.clean_content\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA=cleaning(QA)\n",
    "REC=cleaning(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA['date']=QA.날짜.apply(lambda x: datetime.strptime(''.join(x.split('.')[:3]),'%Y%m%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Date_count=QA.groupby('date').count().loc[:,'본문']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chart_studio.plotly.plotly as py\n",
    "import cufflinks as cf\n",
    "cf.go_offline(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Date_count.iplot(kind='bar',colors='Red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Date_count.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as font_manager\n",
    "\n",
    "font_list = font_manager.findSystemFonts(fontpaths=None, fontext='ttf')\n",
    "\n",
    "# 전체개수\n",
    "print(len(font_list)) \n",
    "\n",
    "# 처음 10개만 출력\n",
    "#font_list[:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "def show_contents_length(DF):\n",
    "    font_name = font_manager.FontProperties(fname='/System/Library/Fonts/Supplemental/Arial Narrow Bold Italic.ttf'\n",
    "                                           ).get_name()\n",
    "    rc('font', family=font_name,size=15)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    print('컨텐츠의 최대 길이 :',max(len(l) for l in  DF.contents))\n",
    "    print('컨텐츠의 평균 길이 :',sum(map(len, QA.contents))/len(DF.contents))\n",
    "    plt.hist([len(s) for s in DF.contents], bins=50,color='#d62728')\n",
    "    plt.xlabel('length of contents')\n",
    "    plt.ylabel('number of contents')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_contents_length(QA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokeninzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokeninzing(DF):\n",
    "    stopword=[ '와인','마시다','하다','있다','어제','이기',\n",
    "              '댓글', '대하다','클릭', '드리다','체계',\n",
    "               '댓글', '글','답변','소통','등업',\n",
    "               '이렇다','대부분','그렇다','그러다',\n",
    "                '감사', '되다', '등급', '기본', '안내', '규정', '체계'\n",
    "               '와쌉','계시다','사람','읽다','음',\n",
    "               '가능','가다','가요','가져가다','가지다','그러다',' ㅂ','ㅁ','안녕','안녕하세요']\n",
    "    \n",
    "    kiwi = Kiwi(num_workers=16)\n",
    "    kiwi.prepare()\n",
    "    E=[]\n",
    "    e=[]\n",
    "    for each_doc in kiwi.analyze(DF['contents'], top_n=1):\n",
    "        for each_word in each_doc[0][0]:\n",
    "            if each_word[0] not in stopword:\n",
    "                if ('VV' in each_word[1]) or ('VA' in each_word[1]):\n",
    "                    word=each_word[0] + '다'\n",
    "                    if word not in stopword:\n",
    "                        e.append(word)\n",
    "                if ('NNG' in each_word[1]) or ('NNP' in each_word[1]):\n",
    "                    e.append(each_word[0])\n",
    "                if each_word[0] =='리딩':\n",
    "                    e.append('브'+each_word[0])\n",
    "                if each_word[0] =='페어':\n",
    "                    e.append(each_word[0]+'링')\n",
    "            else:\n",
    "                pass\n",
    "        E.append(e)\n",
    "        e=[]\n",
    "    temp_title=E\n",
    "        \n",
    "#     temp_title = [[each_word[0] if ('NNG' in each_word[1]) or ('NNP' in each_word[1])\n",
    "#                   else each_word[0] + '다' if ('VV' in each_word[1]) or ('VA' in each_word[1])\n",
    "#                   else None for each_word in each_doc[0][0]]\n",
    "#                   for each_doc in kiwi.analyze(DF['contents'], top_n=1)]\n",
    "    target_title = [[each_word for each_word in each_doc if each_word] for each_doc in temp_title]\n",
    "    DF['token']=target_title\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "\n",
    "\n",
    "def soytokenizer(DF):\n",
    "    S=DF.contents\n",
    "    corpus = DoublespaceLineCorpus(S, iter_sent=True)\n",
    "    word_extractor = WordExtractor(min_frequency=2,\n",
    "        min_cohesion_forward=0.5, \n",
    "        min_right_branching_entropy=0.0\n",
    "    )\n",
    "    word_extractor.train(S) # list of str or like\n",
    "    words = word_extractor.extract()\n",
    "    \n",
    "    noun_extractor = LRNounExtractor_v2(verbose=True)\n",
    "    noun_extractor.train(S) \n",
    "    nouns = noun_extractor.train_extract(S)\n",
    "\n",
    "    cohesion_score = {word:score.cohesion_forward for word, score in words.items() if 1.0 > score.cohesion_forward >=0.8}\n",
    "    score = {word:score.cohesion_forward for word, score in nouns.items() if 1.0 > int(score['score']) >=0.6}\n",
    "    \n",
    "    Wtokenizer = MaxScoreTokenizer(scores=cohesion_score)  \n",
    "    Ntokenizer = MaxScoreTokenizer(scores=score)\n",
    "    return Wtokenizer,Ntokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA['more_clear']=QA.contents.apply(lambda x: re.sub('[^ㄱ-ㅎ ㅏ-ㅣ가-힣A-Za-z]+',' ',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WT,T=soytokenizer(QA)\n",
    "QA['soytoken']=QA.more_clear.apply(lambda x: T.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA=Tokeninzing(QA)\n",
    "REC=Tokeninzing(REC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA['corpus']=QA.token.apply(lambda x: ' '.join(x))\n",
    "REC['corpus']=REC.token.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA['doc_len']=QA.contents.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "QA.soytoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_with_word(word):\n",
    "    l=re.findall('(ㅠㅠ)',word)\n",
    "    L=re.findall('(ㅜㅜ)',word)\n",
    "    i=re.findall('(\\?\\?)',word)\n",
    "    return len(l)+len(i)+len(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "QA['pain']=QA.contents.apply(lambda x :'pain' if get_text_with_word(x) != 0 else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAIN=QA[QA.pain!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(DOCLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCLEN=QA[QA.doc_len>165] #2720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "    \n",
    "def plot_2d_graph(vocabs, xs, ys):\n",
    "    plt.figure(figsize=(20 ,20))\n",
    "    font_name = font_manager.FontProperties(fname='c:\\\\windows\\\\fonts\\\\nanumbarungothic-yethangul.ttf',\n",
    "                                           ).get_name()\n",
    "    rc('font', family=font_name)\n",
    "    rc('font', size=15)\n",
    "    plt.scatter(xs, ys, marker = 'o')\n",
    "    for i, v in enumerate(vocabs):\n",
    "        plt.annotate(v, xy=(xs[i], ys[i]))\n",
    "\n",
    "def Token2vec(DF,mincount):\n",
    "    model = Word2Vec(sentences = DF.soytoken, min_count = mincount, workers = 6, sg = 0)\n",
    "    word_vectors = model.wv\n",
    "    pca = PCA(n_components=2)\n",
    "    vocabs = list(model.wv.index_to_key)\n",
    "    word_vocab_list = [model.wv[v] for v in vocabs]\n",
    "    xys = pca.fit_transform(word_vocab_list)\n",
    "    xs = xys[:,0]\n",
    "    ys = xys[:,1]\n",
    "    plot_2d_graph(vocabs, xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Token2vec(PAIN,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA.contents[7174]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA.token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "tfidv,tfidf=TFIDF(QA)\n",
    "lda=LatentDirichletAllocation(n_components=5)\n",
    "lda.fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_certain_words(corpus, sparse_matrix, drop_words):\n",
    "    drop_words_index = [np.where(corpus == word)[0][0] for word in drop_words]\n",
    "    to_keep = sorted(set(range(sparse_matrix.shape[1])) - set(drop_words_index))\n",
    "    corpus = corpus[to_keep]\n",
    "    sparse_matrix = sparse_matrix[:, to_keep]\n",
    "    return corpus, sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        important_words = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        print(\"Topic %d:\\\" % topic_idx\")\n",
    "        print(\",\".join(important_words))\n",
    "        topics.append(important_words)\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_topics(lda,tfidv.get_feature_names(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[QA.corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "def TFIDF(DF):\n",
    "    tfidv = TfidfVectorizer(min_df=0.1).fit(DF.corpus)\n",
    "    #tfidf = TfidfVectorizer(max_features = 100, max_df=0.95, min_df=0).fit_transform(DF.corpus)# 상위 100개\n",
    "    TFIDF=tfidv.transform(DF.corpus)\n",
    "    #data_array = TFIDF.toarray()\n",
    "    #text=tfidv.get_feature_names()\n",
    "    return tfidv,TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF(QA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closer_look(df, topic_num, content, limit=40):\n",
    "    each_topic_df = df[df['topic label'] == topic_num]\n",
    "    print(each_topic_df[['topic prob', '제목', '본문', '댓글']].sort_values(by='topic prob', ascending=False)[content][:limit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_labeling(QA,TFIDF(QA)[1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "from gensim import corpora\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token=QA.token\n",
    "dictionary = corpora.Dictionary(Token)\n",
    "corpus = [dictionary.doc2bow(text) for text in Token]\n",
    "\n",
    "NUM_TOPICS = 5 #20개의 토픽, k=20\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
