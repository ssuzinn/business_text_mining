{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from kiwipiepy import Kiwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(name):\n",
    "    with open(f'crawl_result/{name}.json','r',encoding='utf-8')as f:\n",
    "        data=json.load(f)\n",
    "        DF=pd.DataFrame(data['data'])\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA=data_load('wineQ&A_text')\n",
    "rec=data_load('wine_recommend_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    text=emoji_pattern.sub(r'', text)\n",
    "    re_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),|]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    text = re.sub(re_pattern, 'url', text)\n",
    "    text = re.sub('[+]', ',', text)\n",
    "    text=re.sub('[^ ㄱ-ㅣ가-힣A-Za-z0-9!?.,~[]]+',' ',text)\n",
    "    text=re.sub('[\\\\s *]',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(DF):\n",
    "    DF['clean_content']=DF.본문.apply(lambda x:clean_text(x))\n",
    "    DF['clean_title']=DF.제목.apply(lambda x:clean_text(x))\n",
    "    DF['contents']=DF.clean_title+DF.clean_content\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA=cleaning(QA)\n",
    "REC=cleaning(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D=[]\\n\",\n",
    "    \"for date in list(set(QA.날짜.tolist())):\\n\",\n",
    "    \"    c=0\\n\",\n",
    "    \"    for ind in QA.index:\\n\",\n",
    "    \"        print(''.join(date.split('.')[:3]))\\n\",\n",
    "    \"        if ''.join(date.split('.')[:3]) == ''.join(QA.loc[ind,'날짜'].split('.')[:3]):\\n\",\n",
    "    \"            c+=1\\n\",\n",
    "    \"    D.append({date:c})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"QA=Tokeninzing(QA)\\n\",\n",
    "    \"REC=Tokeninzing(REC)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"QA['corpus']=QA.token.apply(lambda x: ' '.join(x))\\n\",\n",
    "    \"REC['corpus']=REC.token.apply(lambda x: ' '.join(x))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"import re\\n\",\n",
    "    \"QA['pain']=QA.contents.apply(lambda x :'pain' if len(re.findall('(ㅠㅠ)',x))!=0 else '')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"P=QA[QA.pain!='']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\\n\",\n",
    "    \"from matplotlib import font_manager, rc\\n\",\n",
    "    \"\\n\",\n",
    "    \"def show_contents_length(DF):\\n\",\n",
    "    \"    font_name = font_manager.FontProperties(fname='C:/Windows/Fonts/NanumBarunGothic-YetHangul.ttf').get_name()\\n\",\n",
    "    \"    rc('font', family=font_name)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    plt.figure(figsize=(10, 8))\\n\",\n",
    "    \"    print('컨텐츠의 최대 길이 :',max(len(l) for l in  DF.contents))\\n\",\n",
    "    \"    print('컨텐츠의 평균 길이 :',sum(map(len, QA.contents))/len(DF.contents))\\n\",\n",
    "    \"    plt.hist([len(s) for s in QA.contents], bins=50)\\n\",\n",
    "    \"    plt.xlabel('length of samples')\\n\",\n",
    "    \"    plt.ylabel('number of samples')\\n\",\n",
    "    \"    plt.show()\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token2vec(QA,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"# from wordcloud import WordCloud\\n\",\n",
    "    \"# import matplotlib.pyplot as plt\\n\",\n",
    "    \"\\n\",\n",
    "    \"# wordclouds=WordCloud(width=800,height=800,background_color='white',colormap='Greens')\\n\",\n",
    "    \"# from collections import Counter\\n\",\n",
    "    \"# count=Counter(text)\\n\",\n",
    "    \"# fig=plt.figure(figsize=(10,10))\\n\",\n",
    "    \"# plt.imshow(wordclouds.to_array())\\n\",\n",
    "    \"# plt.show()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"def drop_certain_words(corpus, sparse_matrix, drop_words):\\n\",\n",
    "    \"    drop_words_index = [np.where(corpus == word)[0][0] for word in drop_words]\\n\",\n",
    "    \"    to_keep = sorted(set(range(sparse_matrix.shape[1])) - set(drop_words_index))\\n\",\n",
    "    \"    corpus = corpus[to_keep]\\n\",\n",
    "    \"    sparse_matrix = sparse_matrix[:, to_keep]\\n\",\n",
    "    \"    return corpus, sparse_matrix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list, TFIDF=drop_certain_words(np.array(TFIDF(QA)[0].get_feature_names()),\n",
    "                                     TFIDF(QA)[1],['와인','마시다','하다','있다'\n",
    "                                                   '댓글', '안내', '고수','답변','소통',\n",
    "                                                '이렇다','대부분','그렇다','그러다',\n",
    "                                                   '와쌉','계시다','사람','읽다',\n",
    "                                                   '가능','가다','가요','가져가다','가지다',\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\\n\",\n",
    "    \"from sklearn.feature_extraction.text import CountVectorizer\\n\",\n",
    "    \"from sklearn.feature_extraction.text import TfidfVectorizer\\n\",\n",
    "    \"\\n\",\n",
    "    \"def TFIDF(DF):\\n\",\n",
    "    \"    tfidv = TfidfVectorizer(min_df=0.01).fit(DF.corpus)\\n\",\n",
    "    \"    #tfidf = TfidfVectorizer(max_features = 100, max_df=0.95, min_df=0).fit_transform(DF.corpus)# 상위 100개\\n\",\n",
    "    \"    TFIDF=tfidv.transform(DF.corpus)\\n\",\n",
    "    \"    #data_array = TFIDF.toarray()\\n\",\n",
    "    \"    #text=tfidv.get_feature_names()\\n\",\n",
    "    \"    return tfidv,TFIDF\\n\",\n",
    "    \"\\n\",\n",
    "    \"def Tsne(tfidv,TFIDF,perplexity,n_classes):\\n\",\n",
    "    \"    colors = 'firebrick darksalmon lightseagreen'.split()\\n\",\n",
    "    \"    tsne = TSNE(n_components=2, n_iter=10000, verbose=1 ,perplexity= perplexity)\\n\",\n",
    "    \"    Z = tsne.fit_transform(TFIDF.toarray().T)\\n\",\n",
    "    \"    print(Z[0:5])\\n\",\n",
    "    \"    print('Top words: ',len(Z))\\n\",\n",
    "    \"    tfidf_dict = tfidv.get_feature_names()\\n\",\n",
    "    \"    plt.figure(figsize=(10, 8))\\n\",\n",
    "    \"    plt.scatter(Z[:,0], Z[:,1])\\n\",\n",
    "    \"    for i in range(len(tfidf_dict)):\\n\",\n",
    "    \"        plt.annotate(s=tfidf_dict[i].encode(\\\"utf8\\\").decode(\\\"utf8\\\"), xy=(Z[i,0], Z[i,1]))\\n\",\n",
    "    \"    plt.draw()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"#     for c in range(n_classes):\\n\",\n",
    "    \"#         idx = np.where(data == c)[0]\\n\",\n",
    "    \"#         Z_ = Z[idx]\\n\",\n",
    "    \"#         p.scatter(z_[:,0], z_[:,1], fill_color=colors[c], line_color=colors[c])    \\n\",\n",
    "    \"#     show(p)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\\n\",\n",
    "    \"from sklearn.decomposition import PCA\\n\",\n",
    "    \"\\n\",\n",
    "    \"def plot_2d_graph(vocabs, xs, ys):\\n\",\n",
    "    \"    plt.figure(figsize=(20 ,20))\\n\",\n",
    "    \"    font_name = font_manager.FontProperties(fname='C:/Windows/Fonts/NanumBarunGothic-YetHangul.ttf').get_name()\\n\",\n",
    "    \"    rc('font', family=font_name)\\n\",\n",
    "    \"    rc('font', size=15)\\n\",\n",
    "    \"    plt.scatter(xs, ys, marker = 'o')\\n\",\n",
    "    \"    for i, v in enumerate(vocabs):\\n\",\n",
    "    \"        plt.annotate(v, xy=(xs[i], ys[i]))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"def Token2vec(DF,mincount):\\n\",\n",
    "    \"    model = Word2Vec(sentences = DF.token, vector_size = 3, min_count = mincount, workers = 6, sg = 0)\\n\",\n",
    "    \"    word_vectors = model.wv\\n\",\n",
    "    \"    pca = PCA(n_components=2)\\n\",\n",
    "    \"    vocabs = list(model.wv.index_to_key)\\n\",\n",
    "    \"    word_vocab_list = [model.wv[v] for v in vocabs]\\n\",\n",
    "    \"    xys = pca.fit_transform(word_vocab_list)\\n\",\n",
    "    \"    xs = xys[:,0]\\n\",\n",
    "    \"    ys = xys[:,1]\\n\",\n",
    "    \"    plot_2d_graph(vocabs, xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kiwi = Kiwi(num_workers=16)\\n\",\n",
    "    \"kiwi.prepare()\\n\",\n",
    "    \"T=[[tt[0] if (tt[1] in ['NNG','NNP','SW']) and (tt[0] not in ['와인','글','댓글','등업','답변','질문','있다','되다','안녕','하다','마시','되다',\\n\",\n",
    "    \"                                                                   '등급','소통','감사','안내','클릭'\\n\",\n",
    "    \"                                                                ]) and (len(tt[0])>1)  \\n\",\n",
    "    \"               else None for tt in t[0][0]]\\n\",\n",
    "    \"            for t in kiwi.analyze(P)]\\n\",\n",
    "    \"\\n\",\n",
    "    \"T\\n\",\n",
    "    \"target_title = [[each_word for each_word in each_doc if each_word] for each_doc in T]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokeninzing(DF):\\n\",\n",
    "    \"    kiwi = Kiwi(num_workers=16)\\n\",\n",
    "    \"    kiwi.prepare()\\n\",\n",
    "    \"    temp_title = [[each_word[0] if ('NNG' in each_word[1]) or ('NNP' in each_word[1])\\n\",\n",
    "    \"                   else each_word[0] + '다' if ('VV' in each_word[1]) or ('VA' in each_word[1])\\n\",\n",
    "    \"                   else None for each_word in each_doc[0][0]]\\n\",\n",
    "    \"                  for each_doc in kiwi.analyze(DF['contents'], top_n=1)]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    target_title = [[each_word for each_word in each_doc if each_word] for each_doc in temp_title]\\n\",\n",
    "    \"    DF['token']=target_title\\n\",\n",
    "    \"    return DF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "from gensim import corpora\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=['와인','마시다','하다','있다',\n",
    "  '댓글', '안내', '고수','답변','소통',\n",
    "'이렇다','대부분','그렇다','그러다',\n",
    "'와쌉','계시다','사람','읽다',\n",
    "'가능','가다','가요','가져가다','가지다'\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in QA.token:\n",
    "    for tt in t:\n",
    "        if tt not in stopwords:\n",
    "            token.append(tt)\n",
    "    Token.append(token)\n",
    "    token=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(Token)\n",
    "corpus = [dictionary.doc2bow(text) for text in Token]\n",
    "\n",
    "NUM_TOPICS = 6 #20개의 토픽, k=20\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
